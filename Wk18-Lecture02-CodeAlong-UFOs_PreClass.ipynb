{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc021ffa",
   "metadata": {},
   "source": [
    "# Wk18 Lecture02 CodeAlong: UFOs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20b0a55",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5eb627",
   "metadata": {},
   "source": [
    "- By the end of this CodeAlong, students will be able to:\n",
    "   - Calculate time series statistics (rolling mean/std/diff/pct_change\n",
    "   - Perform feature engineering for time series EDA \n",
    "   - Aggregate time series using date parts to answer stakeholder questions.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3202236",
   "metadata": {},
   "source": [
    "# üïπÔ∏èPart 1) Preparing Irregular-Interval Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60721cfc",
   "metadata": {},
   "source": [
    "### Overview from Last Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c7c4cf",
   "metadata": {},
   "source": [
    "- 1) [ ] Convert the dates & times to a single column (if needed).\n",
    "- 2) [ ] Convert the datetime column  (most likely a string) to a datetime data type.\n",
    "- 3) [ ] Set the datetime column as the Series/DataFrame index\n",
    "- 4) [ ] Resample the time series to the desired/correct frequency using the desired/correct aggregation method.\n",
    "- 5) [ ] Impute null values (if required)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1faf78",
   "metadata": {},
   "source": [
    "### UFO Sightings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8336237c",
   "metadata": {},
   "source": [
    "- UFO Sightings: https://www.kaggle.com/datasets/NUFORC/ufo-sightings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dd25599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as mticks\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import missingno as miss\n",
    "import datetime as dt\n",
    "import statsmodels.tsa.api as tsa\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "264ae028",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration (seconds)</th>\n",
       "      <th>duration (hours/min)</th>\n",
       "      <th>comments</th>\n",
       "      <th>date posted</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10/10/1949 20:30</td>\n",
       "      <td>san marcos</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>2700</td>\n",
       "      <td>45 minutes</td>\n",
       "      <td>This event took place in early fall around 194...</td>\n",
       "      <td>4/27/2004</td>\n",
       "      <td>29.8830556</td>\n",
       "      <td>-97.941111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/10/1949 21:00</td>\n",
       "      <td>lackland afb</td>\n",
       "      <td>tx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>light</td>\n",
       "      <td>7200</td>\n",
       "      <td>1-2 hrs</td>\n",
       "      <td>1949 Lackland AFB&amp;#44 TX.  Lights racing acros...</td>\n",
       "      <td>12/16/2005</td>\n",
       "      <td>29.38421</td>\n",
       "      <td>-98.581082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10/10/1955 17:00</td>\n",
       "      <td>chester (uk/england)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gb</td>\n",
       "      <td>circle</td>\n",
       "      <td>20</td>\n",
       "      <td>20 seconds</td>\n",
       "      <td>Green/Orange circular disc over Chester&amp;#44 En...</td>\n",
       "      <td>1/21/2008</td>\n",
       "      <td>53.2</td>\n",
       "      <td>-2.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10/10/1956 21:00</td>\n",
       "      <td>edna</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>circle</td>\n",
       "      <td>20</td>\n",
       "      <td>1/2 hour</td>\n",
       "      <td>My older brother and twin sister were leaving ...</td>\n",
       "      <td>1/17/2004</td>\n",
       "      <td>28.9783333</td>\n",
       "      <td>-96.645833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10/10/1960 20:00</td>\n",
       "      <td>kaneohe</td>\n",
       "      <td>hi</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>900</td>\n",
       "      <td>15 minutes</td>\n",
       "      <td>AS a Marine 1st Lt. flying an FJ4B fighter/att...</td>\n",
       "      <td>1/22/2004</td>\n",
       "      <td>21.4180556</td>\n",
       "      <td>-157.803611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80327</th>\n",
       "      <td>9/9/2013 21:15</td>\n",
       "      <td>nashville</td>\n",
       "      <td>tn</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>600</td>\n",
       "      <td>10 minutes</td>\n",
       "      <td>Round from the distance/slowly changing colors...</td>\n",
       "      <td>9/30/2013</td>\n",
       "      <td>36.1658333</td>\n",
       "      <td>-86.784444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80328</th>\n",
       "      <td>9/9/2013 22:00</td>\n",
       "      <td>boise</td>\n",
       "      <td>id</td>\n",
       "      <td>us</td>\n",
       "      <td>circle</td>\n",
       "      <td>1200</td>\n",
       "      <td>20 minutes</td>\n",
       "      <td>Boise&amp;#44 ID&amp;#44 spherical&amp;#44 20 min&amp;#44 10 r...</td>\n",
       "      <td>9/30/2013</td>\n",
       "      <td>43.6136111</td>\n",
       "      <td>-116.202500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80329</th>\n",
       "      <td>9/9/2013 22:00</td>\n",
       "      <td>napa</td>\n",
       "      <td>ca</td>\n",
       "      <td>us</td>\n",
       "      <td>other</td>\n",
       "      <td>1200</td>\n",
       "      <td>hour</td>\n",
       "      <td>Napa UFO&amp;#44</td>\n",
       "      <td>9/30/2013</td>\n",
       "      <td>38.2972222</td>\n",
       "      <td>-122.284444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80330</th>\n",
       "      <td>9/9/2013 22:20</td>\n",
       "      <td>vienna</td>\n",
       "      <td>va</td>\n",
       "      <td>us</td>\n",
       "      <td>circle</td>\n",
       "      <td>5</td>\n",
       "      <td>5 seconds</td>\n",
       "      <td>Saw a five gold lit cicular craft moving fastl...</td>\n",
       "      <td>9/30/2013</td>\n",
       "      <td>38.9011111</td>\n",
       "      <td>-77.265556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80331</th>\n",
       "      <td>9/9/2013 23:00</td>\n",
       "      <td>edmond</td>\n",
       "      <td>ok</td>\n",
       "      <td>us</td>\n",
       "      <td>cigar</td>\n",
       "      <td>1020</td>\n",
       "      <td>17 minutes</td>\n",
       "      <td>2 witnesses 2  miles apart&amp;#44 Red &amp;amp; White...</td>\n",
       "      <td>9/30/2013</td>\n",
       "      <td>35.6527778</td>\n",
       "      <td>-97.477778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80332 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               datetime                  city state country     shape  \\\n",
       "0      10/10/1949 20:30            san marcos    tx      us  cylinder   \n",
       "1      10/10/1949 21:00          lackland afb    tx     NaN     light   \n",
       "2      10/10/1955 17:00  chester (uk/england)   NaN      gb    circle   \n",
       "3      10/10/1956 21:00                  edna    tx      us    circle   \n",
       "4      10/10/1960 20:00               kaneohe    hi      us     light   \n",
       "...                 ...                   ...   ...     ...       ...   \n",
       "80327    9/9/2013 21:15             nashville    tn      us     light   \n",
       "80328    9/9/2013 22:00                 boise    id      us    circle   \n",
       "80329    9/9/2013 22:00                  napa    ca      us     other   \n",
       "80330    9/9/2013 22:20                vienna    va      us    circle   \n",
       "80331    9/9/2013 23:00                edmond    ok      us     cigar   \n",
       "\n",
       "      duration (seconds) duration (hours/min)  \\\n",
       "0                   2700           45 minutes   \n",
       "1                   7200              1-2 hrs   \n",
       "2                     20           20 seconds   \n",
       "3                     20             1/2 hour   \n",
       "4                    900           15 minutes   \n",
       "...                  ...                  ...   \n",
       "80327                600           10 minutes   \n",
       "80328               1200           20 minutes   \n",
       "80329               1200                 hour   \n",
       "80330                  5            5 seconds   \n",
       "80331               1020           17 minutes   \n",
       "\n",
       "                                                comments date posted  \\\n",
       "0      This event took place in early fall around 194...   4/27/2004   \n",
       "1      1949 Lackland AFB&#44 TX.  Lights racing acros...  12/16/2005   \n",
       "2      Green/Orange circular disc over Chester&#44 En...   1/21/2008   \n",
       "3      My older brother and twin sister were leaving ...   1/17/2004   \n",
       "4      AS a Marine 1st Lt. flying an FJ4B fighter/att...   1/22/2004   \n",
       "...                                                  ...         ...   \n",
       "80327  Round from the distance/slowly changing colors...   9/30/2013   \n",
       "80328  Boise&#44 ID&#44 spherical&#44 20 min&#44 10 r...   9/30/2013   \n",
       "80329                                       Napa UFO&#44   9/30/2013   \n",
       "80330  Saw a five gold lit cicular craft moving fastl...   9/30/2013   \n",
       "80331  2 witnesses 2  miles apart&#44 Red &amp; White...   9/30/2013   \n",
       "\n",
       "         latitude  longitude   \n",
       "0      29.8830556  -97.941111  \n",
       "1        29.38421  -98.581082  \n",
       "2            53.2   -2.916667  \n",
       "3      28.9783333  -96.645833  \n",
       "4      21.4180556 -157.803611  \n",
       "...           ...         ...  \n",
       "80327  36.1658333  -86.784444  \n",
       "80328  43.6136111 -116.202500  \n",
       "80329  38.2972222 -122.284444  \n",
       "80330  38.9011111  -77.265556  \n",
       "80331  35.6527778  -97.477778  \n",
       "\n",
       "[80332 rows x 11 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ufo  = pd.read_csv(\"Data/ufos-kaggle/scrubbed.csv\", low_memory=False)\n",
    "ufo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12924d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 80332 entries, 0 to 80331\n",
      "Data columns (total 11 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   datetime              80332 non-null  object \n",
      " 1   city                  80332 non-null  object \n",
      " 2   state                 74535 non-null  object \n",
      " 3   country               70662 non-null  object \n",
      " 4   shape                 78400 non-null  object \n",
      " 5   duration (seconds)    80332 non-null  object \n",
      " 6   duration (hours/min)  80332 non-null  object \n",
      " 7   comments              80317 non-null  object \n",
      " 8   date posted           80332 non-null  object \n",
      " 9   latitude              80332 non-null  object \n",
      " 10  longitude             80332 non-null  float64\n",
      "dtypes: float64(1), object(10)\n",
      "memory usage: 6.7+ MB\n"
     ]
    }
   ],
   "source": [
    "ufo.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b742c5c1",
   "metadata": {},
   "source": [
    ">- 1) [x] Convert the dates & times to a single column (if needed)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28fa993",
   "metadata": {},
   "source": [
    "## Preparing the Datetime Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00469c1",
   "metadata": {},
   "source": [
    "### Converting Date Cols to Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd1c07ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "hour must be in 0..23: 10/11/2006 24:00 present at position 388",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/dojo-env/lib/python3.9/site-packages/dateutil/parser/_parser.py:649\u001b[0m, in \u001b[0;36mparser.parse\u001b[0;34m(self, timestr, default, ignoretz, tzinfos, **kwargs)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 649\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_naive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dojo-env/lib/python3.9/site-packages/dateutil/parser/_parser.py:1235\u001b[0m, in \u001b[0;36mparser._build_naive\u001b[0;34m(self, res, default)\u001b[0m\n\u001b[1;32m   1233\u001b[0m         repl[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m monthrange(cyear, cmonth)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m-> 1235\u001b[0m naive \u001b[38;5;241m=\u001b[39m \u001b[43mdefault\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrepl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res\u001b[38;5;241m.\u001b[39mweekday \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m res\u001b[38;5;241m.\u001b[39mday:\n",
      "\u001b[0;31mValueError\u001b[0m: hour must be in 0..23",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/dojo-env/lib/python3.9/site-packages/pandas/_libs/tslib.pyx:605\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dojo-env/lib/python3.9/site-packages/pandas/_libs/tslibs/parsing.pyx:320\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.parsing.parse_datetime_string\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dojo-env/lib/python3.9/site-packages/dateutil/parser/_parser.py:1368\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(timestr, parserinfo, **kwargs)\u001b[0m\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDEFAULTPARSER\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimestr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dojo-env/lib/python3.9/site-packages/dateutil/parser/_parser.py:651\u001b[0m, in \u001b[0;36mparser.parse\u001b[0;34m(self, timestr, default, ignoretz, tzinfos, **kwargs)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 651\u001b[0m     \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParserError\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignoretz:\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: hour must be in 0..23: 10/11/2006 24:00",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/dojo-env/lib/python3.9/site-packages/pandas/_libs/tslib.pyx:616\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: invalid string coercion to datetime for \"10/11/2006 24:00\" at position 388",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/dojo-env/lib/python3.9/site-packages/dateutil/parser/_parser.py:649\u001b[0m, in \u001b[0;36mparser.parse\u001b[0;34m(self, timestr, default, ignoretz, tzinfos, **kwargs)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 649\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_naive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dojo-env/lib/python3.9/site-packages/dateutil/parser/_parser.py:1235\u001b[0m, in \u001b[0;36mparser._build_naive\u001b[0;34m(self, res, default)\u001b[0m\n\u001b[1;32m   1233\u001b[0m         repl[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m monthrange(cyear, cmonth)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m-> 1235\u001b[0m naive \u001b[38;5;241m=\u001b[39m \u001b[43mdefault\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrepl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res\u001b[38;5;241m.\u001b[39mweekday \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m res\u001b[38;5;241m.\u001b[39mday:\n",
      "\u001b[0;31mValueError\u001b[0m: hour must be in 0..23",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## convert datetime to datetime\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ufo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mufo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdatetime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dojo-env/lib/python3.9/site-packages/pandas/core/tools/datetimes.py:1068\u001b[0m, in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1068\u001b[0m         values \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1069\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39m_constructor(values, index\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mindex, name\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, (ABCDataFrame, abc\u001b[38;5;241m.\u001b[39mMutableMapping)):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dojo-env/lib/python3.9/site-packages/pandas/core/tools/datetimes.py:438\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, tz, unit, errors, infer_datetime_format, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m infer_datetime_format\n\u001b[1;32m    437\u001b[0m utc \u001b[38;5;241m=\u001b[39m tz \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutc\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 438\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m \u001b[43mobjects_to_datetime64ns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdayfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43myearfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myearfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mutc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequire_iso8601\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequire_iso8601\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n\u001b[1;32m    451\u001b[0m     dta \u001b[38;5;241m=\u001b[39m DatetimeArray(result, dtype\u001b[38;5;241m=\u001b[39mtz_to_dtype(tz_parsed))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dojo-env/lib/python3.9/site-packages/pandas/core/arrays/datetimes.py:2177\u001b[0m, in \u001b[0;36mobjects_to_datetime64ns\u001b[0;34m(data, dayfirst, yearfirst, utc, errors, require_iso8601, allow_object, allow_mixed)\u001b[0m\n\u001b[1;32m   2175\u001b[0m order: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m flags\u001b[38;5;241m.\u001b[39mf_contiguous \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2176\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2177\u001b[0m     result, tz_parsed \u001b[38;5;241m=\u001b[39m \u001b[43mtslib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray_to_datetime\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mK\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2179\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mutc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdayfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2182\u001b[0m \u001b[43m        \u001b[49m\u001b[43myearfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myearfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequire_iso8601\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequire_iso8601\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_mixed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_mixed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2185\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2186\u001b[0m     result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mreshape(data\u001b[38;5;241m.\u001b[39mshape, order\u001b[38;5;241m=\u001b[39morder)\n\u001b[1;32m   2187\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOverflowError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   2188\u001b[0m     \u001b[38;5;66;03m# Exception is raised when a part of date is greater than 32 bit signed int\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dojo-env/lib/python3.9/site-packages/pandas/_libs/tslib.pyx:427\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dojo-env/lib/python3.9/site-packages/pandas/_libs/tslib.pyx:683\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dojo-env/lib/python3.9/site-packages/pandas/_libs/tslib.pyx:829\u001b[0m, in \u001b[0;36mpandas._libs.tslib._array_to_datetime_object\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dojo-env/lib/python3.9/site-packages/pandas/_libs/tslib.pyx:819\u001b[0m, in \u001b[0;36mpandas._libs.tslib._array_to_datetime_object\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dojo-env/lib/python3.9/site-packages/pandas/_libs/tslibs/parsing.pyx:320\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.parsing.parse_datetime_string\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dojo-env/lib/python3.9/site-packages/dateutil/parser/_parser.py:1368\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(timestr, parserinfo, **kwargs)\u001b[0m\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser(parserinfo)\u001b[38;5;241m.\u001b[39mparse(timestr, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDEFAULTPARSER\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimestr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dojo-env/lib/python3.9/site-packages/dateutil/parser/_parser.py:651\u001b[0m, in \u001b[0;36mparser.parse\u001b[0;34m(self, timestr, default, ignoretz, tzinfos, **kwargs)\u001b[0m\n\u001b[1;32m    649\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_naive(res, default)\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 651\u001b[0m     \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParserError\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignoretz:\n\u001b[1;32m    654\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_tzaware(ret, res, tzinfos)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: hour must be in 0..23: 10/11/2006 24:00 present at position 388"
     ]
    }
   ],
   "source": [
    "## convert datetime to datetime\n",
    "ufo['datetime'] = pd.to_datetime(ufo['datetime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67538565",
   "metadata": {},
   "source": [
    "#### Handling Errors with pd.to_datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e55b74",
   "metadata": {},
   "source": [
    "- Can use the `errors` argument for pd.to_datetime:\n",
    "    - \"raise\" (default): raise an exception when errors happen\n",
    "    - 'ignore': ignores the errors and returns the original value for that row. \n",
    "        - NOT RECOMMENDED: the entire column will not be datetime.\n",
    "    - 'coerce': convert any bad datetime values to null values (NaT - NotATime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ea5b07",
   "metadata": {},
   "source": [
    ">- **Branch point: we have a choice on how we deal with the bad timestamps.**\n",
    "    -  Do we coerce them, make then null values, and drop them? Potentially losing a lot of data.\n",
    "    - Or do we investigate a bit more to see if we can fix the problem without losing data.\n",
    "    \n",
    "    \n",
    "- Let's see how much data we would lose if we chose to coerce the bad values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2348ccaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##check nulls before coercing errors\n",
    "ufo.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460a3785",
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving a copy of original datetime column as datetime-original\n",
    "ufo['datetime-original'] = ufo['datetime'].copy()\n",
    "ufo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0fdfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a datetime-coerce column using pd.to_Datetime with errors = \"coerce\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd82444",
   "metadata": {},
   "outputs": [],
   "source": [
    "## How many null values did we create?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e167eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## What % of dates became null?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93525fad",
   "metadata": {},
   "source": [
    "#### Acceptable # of Rows Lost, but there is another solution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d13418",
   "metadata": {},
   "outputs": [],
   "source": [
    "## inspecting just the bad rows\n",
    "bad_rows = None\n",
    "bad_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3159cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's try to convert the bad rows again\n",
    "try:\n",
    "    pd.to_datetime(bad_rows['datetime-original'])\n",
    "except Exception as e:\n",
    "    display(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2733cf",
   "metadata": {},
   "source": [
    "> ü§î\"`ParserError: hour must be in 0..23: 10/11/2006 24:00 present at position 0`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd52710",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_rows['datetime-original']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12633237",
   "metadata": {},
   "source": [
    "> Panda's is confused by 24:00. It doesn't know if we mean 0:00 of the NEXT day or if we mean the 11:59 pm (23:59) the same day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71db3e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## replace 24:00 with 23:59\n",
    "ufo['datetime-fixed'] = ufo[\"datetime-original\"].str.replace(\" 24:00\",\" 23:59\")\n",
    "ufo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab75bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "## use the fixed-datetime column to make the datetime col\n",
    "ufo['datetime'] = pd.to_datetime(ufo[\"datetime-fixed\"])\n",
    "ufo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb0be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ufo.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c846e3",
   "metadata": {},
   "source": [
    ">- 2) [x] Convert the datetime column  (most likely a string) to a datetime data type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c58dfe",
   "metadata": {},
   "source": [
    "### Setting datetime index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b461d84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create ufo_ts by setting the datetime index\n",
    "ufo_ts = None\n",
    "ufo_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cade062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop any remaining datetime columns\n",
    "#dt_cols = [c for c in ufo_ts.columns if 'datetime' in c]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a204d944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the index to confirm its datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c567112",
   "metadata": {},
   "source": [
    "- 3) [x] Set the datetime column as the Series/DataFrame index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1c2594",
   "metadata": {},
   "source": [
    "## Let's visualize Our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e3e6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the full dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ecaabe",
   "metadata": {},
   "source": [
    "> Hmmmm.... what are we *trying* to visualize?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4a0b46",
   "metadata": {},
   "source": [
    "### What do we really want to know about UFO's? \n",
    "- Duration of sighting?\n",
    "- Location of sighting?\n",
    "- Number of sightings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a04cec",
   "metadata": {},
   "source": [
    "## Getting Our Time Series of UFO Sightings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34f59b0",
   "metadata": {},
   "source": [
    "- We want to quantify the number of events that occurred within each interval.\n",
    "\n",
    ">- Q: How could we do this? (there's several ways)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3173a553",
   "metadata": {},
   "outputs": [],
   "source": [
    "## A.1) Add a count column with a value of 1 for each row and then use reasmple().sum()\n",
    "demo_ts = ufo_ts.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462ae029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A.2) resample and take sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57fa5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Option B) use .size\n",
    "ufo_ts.resample(\"D\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad188a5a",
   "metadata": {},
   "source": [
    "### Make `ts` from ufo_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d27a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resample as daily data \n",
    "ts = ufo_ts.resample('D').size()\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c0714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the ts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891a0c3c",
   "metadata": {},
   "source": [
    "> Let's keep data from 1950 to present day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642c0036",
   "metadata": {},
   "outputs": [],
   "source": [
    "## keep only 1950 and later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ecfbcc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09867db1",
   "metadata": {},
   "source": [
    "# Part 2) Aggregating Full Dataset Using Date Parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527483ce",
   "metadata": {},
   "source": [
    "## üìù **Stakeholder Questions to Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac5849c",
   "metadata": {},
   "source": [
    "**ANSWER TOGETHER:**\n",
    "- 1) What Month/Year had the most sightings? (and how many sightings were there?)\n",
    "\n",
    "- 2) Which month of the year has the highest number of reported sightings?\n",
    "- 3) Is there a seasonal pattern to UFO sightings? If so, how long is the season?\n",
    "\n",
    "- 4) Which US holiday has the largest number of sightings?\n",
    "___\n",
    "**ANSWER SELECTED Q's IN BREAKOUT ROOMS**\n",
    "\n",
    "- 5) Which year had the highest % increase in sightings compared to previous years? (since 1950)\n",
    "\n",
    "- 6) What day of the week has the highest reported sightings?\n",
    "\n",
    "- 7) At what time of day (hour) do most sightings occur?\n",
    "\n",
    "- 8) Which US state has the most sightings?\n",
    "\n",
    "- 9) Which country had the largest proportion of sightings for the year 2000?\n",
    "\n",
    "- 10) Have the types/shapes of UFO's witness changed over time?\n",
    "    - Tip: use only the 4 most common shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10edfa6d",
   "metadata": {},
   "source": [
    "### Making `eda_df` for answering questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d52077",
   "metadata": {},
   "outputs": [],
   "source": [
    "## making eda_df with date as a column instead of index\n",
    "eda_df = ufo_ts.reset_index()\n",
    "eda_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612232e9",
   "metadata": {},
   "source": [
    "### Feature Engineering: Date Parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b44dcb6",
   "metadata": {},
   "source": [
    "- Datetime objects have:\n",
    "    - year\n",
    "    - month\n",
    "    - month_name()\n",
    "    - day\n",
    "    - day_name()\n",
    "    - hour\n",
    "    - seconds\n",
    "    \n",
    "- Pandas has a `.dt.` accessor to use datetime methods on an entire column at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47525f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## feature engineering for dates\n",
    "eda_df['year'] = eda_df['datetime'].dt.year\n",
    "eda_df['month'] = eda_df['datetime'].dt.month_name()\n",
    "eda_df['day of month'] = eda_df['datetime'].dt.day\n",
    "eda_df['day of week'] = eda_df['datetime'].dt.day_name()\n",
    "eda_df['hour'] = eda_df['datetime'].dt.hour\n",
    "eda_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e66a9b",
   "metadata": {},
   "source": [
    "> Let's add a \"weekend\" feature that will be True if the day was a Saturday or Sunday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35d69b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's add a weekend feature\n",
    "eda_df['weekend'] = eda_df['day of week'].isin(['Saturday','Sunday'])\n",
    "eda_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32330fca",
   "metadata": {},
   "source": [
    "#### Let's add a column for the decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0628e5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate decade by subtracting the remainder and div by 10\n",
    "eda_df['decade'] = eda_df['year'] - eda_df['year']%10\n",
    "eda_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8352946d",
   "metadata": {},
   "source": [
    "## üïπÔ∏è Answering Stakeholder Questions (Together)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fce8a17",
   "metadata": {},
   "source": [
    "### Making `eda_ts` & `ts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31028536",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making eda_ts with dt index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423fc596",
   "metadata": {},
   "source": [
    "### 1) What Month/Year had the most sightings? (and how many sightings were there?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f356da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make a ts that is resampled to correct freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a642dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the date of the max sightings\n",
    "date_most_ufos = None\n",
    "date_most_ufos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5542912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many sightings?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1efb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the ts and add vertical line at month with most sightings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08c6daa",
   "metadata": {},
   "source": [
    "### 2) Which month of the year has the highest number of reported sightings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bd4974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b94a204",
   "metadata": {},
   "source": [
    "\n",
    "### 3) Is there a seasonal pattern to UFO sightings? If so, how long is the season?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934fe4a1",
   "metadata": {},
   "source": [
    "### Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283ae884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.tsa.api as tsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22fdf26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ts_check_season = ts_m.loc[\"2000\":]\n",
    "ts_check_season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cab871",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the sliced ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76c6785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Use seasonal_decompose and plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc8de40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## separate seasonal component and plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa938fe1",
   "metadata": {},
   "source": [
    "#### Using scipy's find_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421d152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "peaks, props = find_peaks(seasonal, height=seasonal.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720d6a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab5b713",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "peak_dates = seasonal.index[peaks]\n",
    "peak_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddea2d0",
   "metadata": {},
   "source": [
    "### 4) Which US holiday has the largest number of sightings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100422b2",
   "metadata": {},
   "source": [
    "#### Feature Engineering: Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e80106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install holidays\n",
    "import holidays\n",
    "import datetime as dt\n",
    "from holidays import country_holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a4ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an instance of the US country holidays.\n",
    "us_holidays = country_holidays('US')\n",
    "us_holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9217da70",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a test holiday \n",
    "test = \"01/01/2015\"\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f90b9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test the api \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6788f0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map the api's .get method onto the df to get all holidays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd53d3c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Check the unique holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b94435",
   "metadata": {},
   "source": [
    "#### Answer to which holiday has most sightings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed27cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453403a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4548fce5",
   "metadata": {},
   "source": [
    "#### Wait...when did **that** movie come out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314f6266",
   "metadata": {},
   "outputs": [],
   "source": [
    "release_date= '1997-07-03'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb00ce8c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Plot the # of sightings over time and annotate the release date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eca3367",
   "metadata": {},
   "source": [
    "## üèì**Breakout Rooms: Answering Stakeholder Questions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb32f8cc",
   "metadata": {},
   "source": [
    "**Choose 1-2 of the remaining questions and work in breakout rooms to answer them:**\n",
    "- 5) Which year had the highest % increase in sightings compared to previous years?\n",
    "- 6) What day of the week has the highest reported sightings?\n",
    "- 7) At what time of day (hour) do most sightings occur?\n",
    "- 8) Which US state has the most sightings?\n",
    "- 9) Which country had the largest proportion of sightings for the year 2000?\n",
    "- 10) Have the types/shapes of UFO's witness changed over time?\n",
    "    - Tip: use only the 4 most common shapes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f50277",
   "metadata": {},
   "source": [
    "### 5) Which year had the highest % increase in sightings compared to previous years? (since 1950)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2d363e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c33f59e",
   "metadata": {},
   "source": [
    "### 6) What day of the week has the highest reported sightings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1253d143",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_ts['day of week'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e5c3e7",
   "metadata": {},
   "source": [
    "### 6) Which country had the largest proportion of sightings for the year 2000?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273cd18d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "194202ac",
   "metadata": {},
   "source": [
    "### 7) Have the types/shapes of UFO's witness changed over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c1c0aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a91207f",
   "metadata": {},
   "source": [
    "___\n",
    "# Bonus: Plotly Express"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be49b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21a4e82",
   "metadata": {},
   "source": [
    "### Map Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2af13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_df = eda_df.sort_values('decade')\n",
    "eda_df.columns = eda_df.columns.str.strip()\n",
    "eda_df['latitude'] = pd.to_numeric(eda_df['latitude'], errors='coerce')\n",
    "eda_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114f1d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter_geo(data_frame=eda_df, lat='latitude',lon='longitude', animation_frame=\"decade\",\n",
    "              template='ggplot2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aafc1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f452f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dojo-env)",
   "language": "python",
   "name": "dojo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "212.344px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
